{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Hypnos Agent Model Evaluation Notebook üîç\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dmitrykazhdan/HYPNOS/refs/heads/main/assets/hypnos_icon.png\" alt=\"Icon\" width=\"100\"/>  "
      ],
      "metadata": {
        "id": "T9JHNWteKmLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation üîß\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_ywPVA1KsjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount to your Google Drive"
      ],
      "metadata": {
        "id": "2lAb5QtULMxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKVrRqZPKpet"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify whether you have GPUs enabled (Recommended)"
      ],
      "metadata": {
        "id": "wTprXv3NPKUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "FN9qHmI2TcRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "gpu_acceleration = torch.cuda.is_available()\n",
        "print(gpu_acceleration)"
      ],
      "metadata": {
        "id": "CbqPQVeQSDaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gpu_acceleration:\n",
        "  # Required for CMake + CUDA builds\n",
        "  !apt-get -qq install -y cmake build-essential\n",
        "\n",
        "  # Confirm GPU is visible\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "a1FJHN6TPHaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "ZLjWONRrOZN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU-enablement\n",
        "from llama_cpp import Llama\n",
        "print(\"‚úÖ GPU-enabled build!\" if \"n_gpu_layers\" in Llama.__init__.__code__.co_varnames else \"‚ùå CPU-only build!\")"
      ],
      "metadata": {
        "id": "6tDSCDSYOd1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate rouge rouge_score"
      ],
      "metadata": {
        "id": "0NTnYtfkKw8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation üîç"
      ],
      "metadata": {
        "id": "HbIEedaUK5gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify models to load, and the data file path\n",
        "drive_root = \"...\"\n",
        "\n",
        "GGUF_MODEL_NAMES_TO_PATH_DICT = [\n",
        "    (\"Baseline\",    f\"{drive_root}/...gguf\"),\n",
        "    (\"SFT_Quantized\",   f\"{drive_root}/...gguf\"),\n",
        "    (\"DPO_Quantized\",   f\"{drive_root}/...gguf\"),\n",
        "]\n",
        "\n",
        "TEST_JSON = f\"{drive_root}/data/sleep-test-enriched-cleaned.json\"\n",
        "\n",
        "# Set to >0 for subsetting a portion of data\n",
        "# Set to <= 0 for using all data\n",
        "SUBSET = 0"
      ],
      "metadata": {
        "id": "hhS4x_hdLQ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define helper functions for model loading"
      ],
      "metadata": {
        "id": "3qUsOVFxLp1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import evaluate, json, numpy as np, torch, gc, time, os\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    time.sleep(0.3)\n",
        "\n",
        "def prompt(q):\n",
        "    return (\n",
        "        f\"<bos><start_of_turn>user\\n{q} (Respond in one sentence)\"\n",
        "        \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "def load_gguf(path):\n",
        "    return Llama(\n",
        "        model_path=path,\n",
        "        n_ctx=512,\n",
        "        n_threads=os.cpu_count() or 4,\n",
        "        verbose=False,\n",
        "        n_gpu_layers=20 if gpu_acceleration else 0\n",
        "    )\n",
        "\n",
        "def generate(llm, qs):\n",
        "    outs = []\n",
        "    for i, q in enumerate(qs):\n",
        "        prompt_text = prompt(q)\n",
        "        out = llm(prompt_text, max_tokens=128, temperature=0.0, stop=[\"<end_of_turn>\"])\n",
        "        text = out[\"choices\"][0][\"text\"].strip()\n",
        "        print(f\"   sample {i+1}: {text[:120]}\")\n",
        "        outs.append(text)\n",
        "    return outs"
      ],
      "metadata": {
        "id": "Jbfin-8TLK06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define helper functions for prediction evaluation using ROUGE-L scoring"
      ],
      "metadata": {
        "id": "Gv5kPZuqL-st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "def rouge_l(preds, refs):\n",
        "  return rouge.compute(predictions=preds, references=refs)[\"rougeL\"]\n",
        "\n",
        "def rouge_ci(p, r, boot=1000, conf=0.95):\n",
        "    scores = []\n",
        "    n = len(p)\n",
        "    for _ in range(boot):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        scores.append(rouge_l([p[i] for i in idx], [r[i] for i in idx]))\n",
        "    lo, hi = np.percentile(scores, [(1 - conf) * 50, 100 - (1 - conf) * 50])\n",
        "    return lo, hi"
      ],
      "metadata": {
        "id": "_0hO7N8RL7Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're all set to run the evaluation... üöÄ\n",
        "\n",
        "Note: anticipated ROUGE score on SleeQA enriched is ~0.25-0.3\n"
      ],
      "metadata": {
        "id": "kJd5buEvNnMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"\\nüß™ GGUF Model Evaluation\\n\" + \"‚îÄ\"*50)\n",
        "    data = json.load(open(TEST_JSON))\n",
        "    items = [(d[\"question\"], d[\"answer\"]) for d in data]\n",
        "    if SUBSET > 0: items = items[:SUBSET]\n",
        "    Qs, Rs = zip(*items)\n",
        "    print(f\"{len(Qs)} test questions\\n\")\n",
        "\n",
        "    for label, path in GGUF_MODEL_NAMES_TO_PATH_DICT:\n",
        "        cleanup()\n",
        "        llm = load_gguf(path)\n",
        "        preds = generate(llm, Qs)\n",
        "        score = rouge_l(preds, Rs)\n",
        "        lo, hi = rouge_ci(preds, Rs, boot=500)\n",
        "        avg_len = np.mean([len(p.split()) for p in preds])\n",
        "        print(f\"\\n{label:<12} ROUGE‚ÄëL {score:.4f} \"\n",
        "              f\"[{lo:.4f}, {hi:.4f}]  len={avg_len:.1f}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Eq2Khm5fMFqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}