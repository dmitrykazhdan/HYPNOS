{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WmOAXHm2CMSK",
        "QihGB-fIDJO5",
        "9noUHN9HHm6m",
        "kUgIPbjAIdsL",
        "GsXZc_8sX1Yl"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Hypnos Agent Model Training Notebook ğŸš€\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dmitrykazhdan/HYPNOS/refs/heads/main/assets/hypnos_icon.png\" alt=\"Icon\" width=\"100\"/>  "
      ],
      "metadata": {
        "id": "v1O0iZfYBnR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup ğŸ§°\n",
        "\n",
        "- Ensure you are using a GPU Runtime, such as a T4 GPU\n",
        "- High-RAM model is highly recommended\n",
        "- For Unsloth-related issues, consult their:\n",
        "  - [Documentation](https://docs.unsloth.ai/)\n",
        "  - [Discord](https://discord.gg/unsloth)"
      ],
      "metadata": {
        "id": "zt9y99BSCqAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation ğŸ”§\n",
        "\n",
        "- Lets go through the core package installation steps\n",
        "- â—We also specify a few GOTCHAs with package compatibility"
      ],
      "metadata": {
        "id": "WmOAXHm2CMSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "_uiWdp2YA3dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This was needed as of 30.07.2025 for a change pushed to trl which broke unsloth functionality\n",
        "!pip install --no-deps --force-reinstall trl==0.19.1"
      ],
      "metadata": {
        "id": "Zi_U1BXGCOUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps --upgrade timm # Needed Gemma 3N"
      ],
      "metadata": {
        "id": "ueNr0BhRCddT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "s7DIo3urChjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "najXGYFyCP4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Finetuning (SFT) âš¡\n",
        "\n",
        "- â— Unsloth occasionally has issues with freeing memory. It is recommended to do a \"Restart Session\" or otherwise cleanup GPU resources between model runs\n",
        "- â³ This section may take 1-2hrs to run\n",
        "- â—Recommendation: make sure it runs end-to-end with `SUBSET=5` to ensure all packages/systems are setup correctly. Then run the full pass with `SUBSET=0`"
      ],
      "metadata": {
        "id": "QihGB-fIDJO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ---------------------------------------------------\n",
        "import unsloth\n",
        "from unsloth import FastModel, is_bfloat16_supported\n",
        "from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig\n",
        "from datasets import Dataset\n",
        "import json, torch, gc, os, shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# See: https://github.com/huggingface/transformers/issues/39427\n",
        "torch._dynamo.config.cache_size_limit = 128\n",
        "torch._dynamo.config.suppress_errors  = True\n"
      ],
      "metadata": {
        "id": "nn5x2NiCDjHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_REPO  = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "# Set to a gdrive of your choice\n",
        "drive_root = \"...\"\n",
        "data_json  = f'{drive_root}/data/sleep-train-enriched.json'\n",
        "\n",
        "# Set training configuration parameters\n",
        "EPOCHS_SFT = 3\n",
        "LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\n",
        "LR = 1e-4\n",
        "SEED = 3407\n",
        "SUBSET     = 0              # >0 debugs with a subset; =0 uses full dataset\n",
        "\n",
        "now     = lambda: datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "cleanup = lambda: (torch.cuda.empty_cache(), gc.collect())\n"
      ],
      "metadata": {
        "id": "wBmYc4dWEtR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOAD BASE (4â€‘bit) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"ğŸ¤– Loading base 4â€‘bit repo â€¦\")\n",
        "model, tok = FastModel.from_pretrained(\n",
        "    BASE_REPO, load_in_4bit=False, full_finetuning=False, max_seq_length=2048\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n",
        "print(\"âœ… Base ready\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ADDÂ LoRA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
        "    finetune_language_layers=True,\n",
        "    finetune_attention_modules=True,\n",
        "    finetune_mlp_modules=True,\n",
        "    bias=\"none\",\n",
        "    random_state=SEED,\n",
        ")\n",
        "print(\"ğŸ”§ LoRA adapters injected\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATASET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "raw = Dataset.from_list(json.load(open(data_json)))\n",
        "def to_chat(ex):\n",
        "    return {\"text\":\n",
        "        f\"<bos><start_of_turn>user\\n{ex['question']}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n{ex['answer']}<end_of_turn>\"\n",
        "    }\n",
        "data = raw.map(to_chat)\n",
        "\n",
        "if SUBSET:\n",
        "    data = data.select(range(min(SUBSET, len(data))))\n",
        "train_ds, val_ds = data.train_test_split(0.2, seed=SEED).values()\n",
        "print(f\"ğŸ“š Trainâ€¯/â€¯val = {len(train_ds)}â€¯/â€¯{len(val_ds)}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SFT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nğŸ¯Â Supervised fineâ€‘tune â€¦\")\n",
        "\n",
        "cfg = SFTConfig(\n",
        "    dataset_text_field          = \"text\",\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    num_train_epochs            = EPOCHS_SFT,\n",
        "    learning_rate               = LR,\n",
        "    warmup_steps                = 10,\n",
        "    logging_steps               = 10,\n",
        "    fp16                        = not is_bfloat16_supported(),\n",
        "    bf16                        = is_bfloat16_supported(),\n",
        "    optim                       = \"adamw_8bit\",\n",
        "    seed                        = SEED,\n",
        "    report_to                   = \"wandb\",\n",
        ")\n",
        "SFTTrainer(model=model, tokenizer=tok, train_dataset=train_ds,\n",
        "           eval_dataset=val_ds, args=cfg).train()"
      ],
      "metadata": {
        "id": "2glvFKvmDhDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SAVEÂ ADAPTERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "stamp       = now()\n",
        "adapter_dir = f\"hypnos_adapters_{stamp}\"\n",
        "model.save_pretrained(adapter_dir); tok.save_pretrained(adapter_dir)\n",
        "shutil.copytree(adapter_dir, f\"{drive_root}/{adapter_dir}\")\n",
        "print(\"ğŸ’¾  LoRAâ€‘only checkpoint saved:\", adapter_dir)\n",
        "\n",
        "print(\"\\nğŸ‰Â Training complete\")\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "eGEra1y3FeAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning with AI Feedback (RLAIF) ğŸ¤–\n",
        "\n",
        "- Here we use the Direct Policy Optimisation RLAIF approach from Unsloth"
      ],
      "metadata": {
        "id": "9noUHN9HHm6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastModel, PatchDPOTrainer, is_bfloat16_supported\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import Dataset\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Common GOTCHA with recursion limit issue in Unsloth\n",
        "torch._dynamo.config.cache_size_limit = 128\n",
        "# Patch Unsloth's DPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "drive_root   = \"/content/gdrive/...\"\n",
        "adapter_dir  = f\"/content/hypnos_adapters_20250802_081519\"                      # â† your saved LoRA dir\n",
        "base_model_id    = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "DPO_DATASET  = f\"{drive_root}/data/sleep-train-dpo-enriched.json\"               # replace with your DPO JSON file\n",
        "max_samples = 0         # Test subset by setting to >0\n",
        "\n",
        "\n",
        "# 1. Load base model + tokenizer\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    base_model_id,                     # same base used during original SFT\n",
        "    load_in_4bit=False,\n",
        "    full_finetuning=False,\n",
        "    max_seq_length=2048\n",
        ")\n",
        "\n",
        "# 2. Inject LoRA structure into the model (important!)\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    finetune_language_layers=True,\n",
        "    finetune_attention_modules=True,\n",
        "    finetune_mlp_modules=True,\n",
        "    bias=\"none\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "# 3. Load previously fine-tuned LoRA weights (SFT checkpoint)\n",
        "model.load_adapter(adapter_dir, adapter_name=\"default\")\n"
      ],
      "metadata": {
        "id": "MSy9qefuVgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# === Load and validate your preference data ===\n",
        "with open(DPO_DATASET) as f:\n",
        "    preference_data = json.load(f)\n",
        "\n",
        "# âœ… Rename \"question\" â†’ \"prompt\"\n",
        "for sample in preference_data:\n",
        "    sample[\"prompt\"] = sample.pop(\"question\")\n",
        "\n",
        "print(f\"ğŸ“Š Created {len(preference_data)} preference pairs\")\n",
        "if preference_data:\n",
        "    print(f\"ğŸ“‹ Sample preference pair:\\n{json.dumps(preference_data[0], indent=2)}\")\n",
        "else:\n",
        "    raise ValueError(\"âŒ No valid preference pairs found!\")\n",
        "\n",
        "# === Create Hugging Face dataset ===\n",
        "preference_dataset = Dataset.from_list(preference_data)\n",
        "print(f\"âœ… Preference dataset created with {len(preference_dataset)} examples\")\n",
        "\n",
        "# === Get correct chat template for your tokenizer (adjust template if needed) ===\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"gemma-3\")\n",
        "\n",
        "# === Apply DPO formatting ===\n",
        "print(\"ğŸ”„ Formatting preference dataset for DPO...\")\n",
        "\n",
        "def apply_dpo_template(example):\n",
        "    # Validate all required fields\n",
        "    if not all(isinstance(example[field], str) and example[field].strip() for field in [\"prompt\", \"chosen\", \"rejected\"]):\n",
        "        return {\n",
        "            \"prompt\": \"\",\n",
        "            \"chosen\": \"\",\n",
        "            \"rejected\": \"\"\n",
        "        }\n",
        "\n",
        "    # Format using chat template\n",
        "    return {\n",
        "        \"prompt\": tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": example[\"prompt\"]}], tokenize=False),\n",
        "        \"chosen\": tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": example[\"prompt\"]}, {\"role\": \"assistant\", \"content\": example[\"chosen\"]}], tokenize=False),\n",
        "        \"rejected\": tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": example[\"prompt\"]}, {\"role\": \"assistant\", \"content\": example[\"rejected\"]}], tokenize=False)\n",
        "    }\n",
        "\n",
        "# Apply formatting (no rename step needed)\n",
        "preference_dataset = preference_dataset.map(\n",
        "    apply_dpo_template,\n",
        "    remove_columns=preference_dataset.column_names,\n",
        "    desc=\"ğŸ§  Applying chat template formatting\"\n",
        ")\n",
        "\n",
        "# Optional sanity check\n",
        "print(\"âœ… Final formatted sample:\")\n",
        "print(preference_dataset[0])\n",
        "\n",
        "\n",
        "if max_samples > 0:\n",
        "  preference_dataset = preference_dataset.select(range(max_samples))\n",
        "\n",
        "split_dataset = preference_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "8876xHJZVMpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DPO CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "config = DPOConfig(\n",
        "    beta                         = 0.01,  # DPO trade-off factor\n",
        "    per_device_train_batch_size  = 2,\n",
        "    per_device_eval_batch_size   = 2,\n",
        "    gradient_accumulation_steps  = 8,\n",
        "    eval_steps                   = 10,\n",
        "    logging_steps                = 5,\n",
        "    num_train_epochs             = 1,\n",
        "    learning_rate                = 1e-5,\n",
        "    lr_scheduler_type            = \"cosine\",\n",
        "    warmup_ratio                 = 0.05,\n",
        "    optim                        = \"adamw_8bit\",\n",
        "    seed                         = 3407,\n",
        "    bf16                         = is_bfloat16_supported(),\n",
        "    fp16                         = not is_bfloat16_supported(),\n",
        "    report_to                    = \"none\"\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model           = model,\n",
        "    ref_model       = None,      # uses frozen copy of model by default\n",
        "    args            = config,\n",
        "    beta            = config.beta,\n",
        "    tokenizer       = None,\n",
        "    train_dataset   = preference_dataset,\n",
        "    max_length      = 2048,\n",
        "    evaluation_strategy=\"steps\",     # or \"epoch\"\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ycoy9cBNsSTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SAVE DPO LoRA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "stamp       = now()\n",
        "dpo_dir     = f\"dpo_hypnos_adapters_{stamp}\"\n",
        "model.save_pretrained(dpo_dir); tokenizer.save_pretrained(dpo_dir)\n",
        "shutil.copytree(dpo_dir, f\"{drive_root}/{dpo_dir}\")\n",
        "print(\"ğŸ’¾  DPO-finetuned LoRA checkpoint saved:\", dpo_dir)\n",
        "\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "w0v77i-JVofD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference ğŸ\n",
        "- Lightweight inference testing to ensure the models were trained / saved / loaded correctly"
      ],
      "metadata": {
        "id": "kUgIPbjAIdsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# Paths\n",
        "BASE_REPO   = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "ADAPTER_DIR = \"/content/dpo_hypnos_adapters_20250802_114929\"\n",
        "\n",
        "# Load base + adapter\n",
        "model, tok = FastModel.from_pretrained(BASE_REPO, load_in_4bit=False)\n",
        "model.load_adapter(ADAPTER_DIR)\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "# Sample prompt\n",
        "prompt = \"<bos><start_of_turn>user\\nHow can I improve my sleep naturally?<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tok(text=prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode result\n",
        "response = tok.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nğŸ“¤ Response:\\n\", response)\n"
      ],
      "metadata": {
        "id": "RlShYgghIjL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting & Quantization ğŸ—³"
      ],
      "metadata": {
        "id": "EMiC-CPDXqmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "GsXZc_8sX1Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "4A08HXzsX20h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to build llamacpp now to enable quantization features\n",
        "\n",
        "Note: Can try `cmake --build . --target quantize --config Release`\n",
        "\n",
        "For a faster, targeted build.\n",
        "\n",
        "âš  But it does not always work across llamacpp versions"
      ],
      "metadata": {
        "id": "JhF2pIB9fh4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# [1] Update and install dependencies\n",
        "sudo apt update\n",
        "sudo apt install -y build-essential cmake\n",
        "\n",
        "# [2] Build llama.cpp\n",
        "cd /content/llama.cpp\n",
        "\n",
        "mkdir build\n",
        "cd build\n",
        "cmake ..\n",
        "cmake --build . --config Release"
      ],
      "metadata": {
        "id": "Orz6eayiesdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure quantization block is installed\n",
        "import os\n",
        "\n",
        "path = \"/content/llama.cpp/build/bin/llama-quantize\"\n",
        "\n",
        "if os.path.isfile(path) or os.path.isdir(path):\n",
        "    print(f\"âœ… Quantization module successfully located...\")\n",
        "else:\n",
        "    print(f\"âŒ File or directory does not exist: {path}\")\n"
      ],
      "metadata": {
        "id": "kommxHC6fNsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting to GGUF & Quantizing"
      ],
      "metadata": {
        "id": "u7zzYoyJYM1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_REPO   = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"      # Set to None if you don't want to save the base model\n",
        "BASE_MODEL_PATH = \"gemma-3n-base-model\"\n",
        "SAVE_BASE_MODEL = False\n",
        "ADAPTER_DIR = \"/content/hypnos_adapters_20250802_081519\"          # Your SFT Adapter\n",
        "DPO_ADAPTER_DIR = \"/content/dpo_hypnos_adapters_20250802_114929\"  # Your DPO Adapter\n",
        "\n",
        "unquantized_sft_gguf = f\"{ADAPTER_DIR}_unquantized.gguf\"\n",
        "quantized_sft_gguf = f\"{ADAPTER_DIR}_quantized.gguf\"\n",
        "\n",
        "unquantized_dpo_gguf = f\"{DPO_ADAPTER_DIR}_unquantized.gguf\"\n",
        "quantized_dpo_gguf = f\"{DPO_ADAPTER_DIR}_quantized.gguf\"\n",
        "\n",
        "quantization = \"q4_k_m\"\n",
        "\n",
        "\n",
        "def load_base_model():\n",
        "  model, tokenizer = FastModel.from_pretrained(\n",
        "      model_name = BASE_REPO,\n",
        "      dtype = None, # None for auto detection\n",
        "      max_seq_length = 1024, # Choose any for long context!\n",
        "      load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "      full_finetuning = False,\n",
        "  )\n",
        "  return model, tokenizer\n"
      ],
      "metadata": {
        "id": "zHEF_D0IXsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "if SAVE_BASE_MODEL:\n",
        "  model, tokenizer = load_base_model()\n",
        "  model.save_pretrained(BASE_MODEL_PATH)  # Local saving\n",
        "  tokenizer.save_pretrained(BASE_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "ptu8CwM0YT80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_BASE_MODEL:\n",
        "  input_model_path = f\"/content/{BASE_MODEL_PATH}\"\n",
        "  output_model_path = f\"/content/{BASE_MODEL_PATH}.gguf\"\n",
        "  !python /content/llama.cpp/convert_hf_to_gguf.py {input_model_path} --outfile {output_model_path}"
      ],
      "metadata": {
        "id": "87xQQigKap1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "for tuned_model_dir in [ADAPTER_DIR, DPO_ADAPTER_DIR]:\n",
        "  model, tokenizer = load_base_model()\n",
        "\n",
        "  model = FastModel.get_peft_model(\n",
        "      model,\n",
        "      r=16,\n",
        "      target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "      lora_alpha=32,\n",
        "      lora_dropout=0.05,\n",
        "      bias=\"none\",\n",
        "  )\n",
        "\n",
        "  # Load the LoRA adapter weights with adapter name\n",
        "  model.load_adapter(tuned_model_dir, adapter_name=\"default\")\n",
        "\n",
        "  # Step 1: Save as full model in proper HF format\n",
        "  full_model_folder = f\"{tuned_model_dir}_merged\"\n",
        "\n",
        "  print(f\"ğŸ’¾ Saving full model to {full_model_folder}...\")\n",
        "  model.save_pretrained_merged(full_model_folder, tokenizer)"
      ],
      "metadata": {
        "id": "XZQMQIv5YhKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Convert to unquantized GGUF using llama.cpp\n",
        "print(f\" Converting to unquantized GGUF...\")\n",
        "\n",
        "sft_full_folder = f\"{ADAPTER_DIR}_merged\"\n",
        "dpo_full_folder = f\"{DPO_ADAPTER_DIR}_merged\"\n",
        "\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py {sft_full_folder} --outfile {unquantized_sft_gguf}\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py {dpo_full_folder} --outfile {unquantized_dpo_gguf}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hubi77wBb9ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run quantization on the extracted model\n",
        "# Using raw bash here vs. subprocess as this might take a while and log output is crucial\n",
        "! \"/content/llama.cpp/build/bin/llama-quantize\" {unquantized_sft_gguf} {quantized_sft_gguf} {quantization}\n",
        "! \"/content/llama.cpp/build/bin/llama-quantize\" {unquantized_dpo_gguf} {quantized_dpo_gguf} {quantization}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mZ3O5Y0ZeCDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the files permanently on the mounted drive"
      ],
      "metadata": {
        "id": "dhVF06cgd6dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify local + mounted paths\n",
        "import os\n",
        "drive_dir = \"/content/gdrive/MyDrive/Files/Personal/Hypnos-Project/content/models\"\n",
        "BASE_MODEL_FILENAME = f\"{BASE_MODEL_PATH}.gguf\"\n",
        "SRC_MODEL_FILENAMES = [unquantized_sft_gguf, quantized_sft_gguf, unquantized_dpo_gguf, quantized_dpo_gguf]\n",
        "MODEL_FILENAMES = [str(os.path.basename(i)) for i in SRC_MODEL_FILENAMES]\n",
        "print(\"Base model filename: \" + BASE_MODEL_FILENAME)\n",
        "print(\"Output model filename: \" + str(MODEL_FILENAMES))\n",
        "\n",
        "BASE_MODEL_GGUF_PATH_OUTPUT = f\"{drive_dir}/{BASE_MODEL_FILENAME}\"\n",
        "DST_MODEL_FILENAMES = [f\"{drive_dir}/{i}\" for i in MODEL_FILENAMES]\n",
        "\n",
        "# Copy models over to mounted dir\n",
        "! cp {BASE_MODEL_FILENAME} {BASE_MODEL_GGUF_PATH_OUTPUT}\n",
        "\n",
        "for (src, dest) in zip(SRC_MODEL_FILENAMES, DST_MODEL_FILENAMES):\n",
        "  print(f\"Copying model {src} over to {dest}...\")\n",
        "  ! cp {src} {dest}"
      ],
      "metadata": {
        "id": "wxt19vTxd40l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You're all done! ğŸ‰ ğŸ‰\n",
        "\n",
        "You should now have base, SFT, and DPO gguf models exported to your GDrive. ğŸ‘\n",
        "\n",
        "Proceed to the Evaluation notebook to test out the extracted gguf... ğŸƒğŸ”§"
      ],
      "metadata": {
        "id": "eUmm4XIpdqXA"
      }
    }
  ]
}