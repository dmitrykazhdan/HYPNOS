{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WmOAXHm2CMSK",
        "QihGB-fIDJO5",
        "9noUHN9HHm6m",
        "kUgIPbjAIdsL",
        "GsXZc_8sX1Yl"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Hypnos Agent Model Training Notebook 🚀\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dmitrykazhdan/HYPNOS/refs/heads/main/assets/hypnos_icon.png\" alt=\"Icon\" width=\"100\"/>  "
      ],
      "metadata": {
        "id": "v1O0iZfYBnR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 🧰\n",
        "\n",
        "- Ensure you are using a GPU Runtime, such as a T4 GPU\n",
        "- High-RAM model is highly recommended\n",
        "- For Unsloth-related issues, consult their:\n",
        "  - [Documentation](https://docs.unsloth.ai/)\n",
        "  - [Discord](https://discord.gg/unsloth)"
      ],
      "metadata": {
        "id": "zt9y99BSCqAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation 🔧\n",
        "\n",
        "- Lets go through the core package installation steps\n",
        "- ❗We also specify a few GOTCHAs with package compatibility"
      ],
      "metadata": {
        "id": "WmOAXHm2CMSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "_uiWdp2YA3dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This was needed as of 30.07.2025 for a change pushed to trl which broke unsloth functionality\n",
        "!pip install --no-deps --force-reinstall trl==0.19.1"
      ],
      "metadata": {
        "id": "Zi_U1BXGCOUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps --upgrade timm # Needed Gemma 3N"
      ],
      "metadata": {
        "id": "ueNr0BhRCddT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "s7DIo3urChjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "najXGYFyCP4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Finetuning (SFT) ⚡\n",
        "\n",
        "- ❗ Unsloth occasionally has issues with freeing memory. It is recommended to do a \"Restart Session\" or otherwise cleanup GPU resources between model runs\n",
        "- ⏳ This section may take 1-2hrs to run\n",
        "- ❗Recommendation: make sure it runs end-to-end with `SUBSET=5` to ensure all packages/systems are setup correctly. Then run the full pass with `SUBSET=0`"
      ],
      "metadata": {
        "id": "QihGB-fIDJO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ---------------------------------------------------\n",
        "import unsloth\n",
        "from unsloth import FastModel, is_bfloat16_supported\n",
        "from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig\n",
        "from datasets import Dataset\n",
        "import json, torch, gc, os, shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# See: https://github.com/huggingface/transformers/issues/39427\n",
        "torch._dynamo.config.cache_size_limit = 128\n",
        "torch._dynamo.config.suppress_errors  = True\n"
      ],
      "metadata": {
        "id": "nn5x2NiCDjHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────── CONFIG ──────────────────────────────────────────────────────────\n",
        "BASE_REPO  = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "# Set to a gdrive of your choice\n",
        "drive_root = \"...\"\n",
        "data_json  = f'{drive_root}/data/sleep-train-enriched.json'\n",
        "\n",
        "# Set training configuration parameters\n",
        "EPOCHS_SFT = 3\n",
        "LORA_R, LORA_ALPHA, LORA_DROPOUT = 16, 32, 0.05\n",
        "LR = 1e-4\n",
        "SEED = 3407\n",
        "SUBSET     = 0              # >0 debugs with a subset; =0 uses full dataset\n",
        "\n",
        "now     = lambda: datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "cleanup = lambda: (torch.cuda.empty_cache(), gc.collect())\n"
      ],
      "metadata": {
        "id": "wBmYc4dWEtR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ─────────── LOAD BASE (4‑bit) ───────────────────────────────────────────────\n",
        "print(\"🤖 Loading base 4‑bit repo …\")\n",
        "model, tok = FastModel.from_pretrained(\n",
        "    BASE_REPO, load_in_4bit=False, full_finetuning=False, max_seq_length=2048\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n",
        "print(\"✅ Base ready\")\n",
        "\n",
        "# ─────────── ADD LoRA ────────────────────────────────────────────────────────\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
        "    finetune_language_layers=True,\n",
        "    finetune_attention_modules=True,\n",
        "    finetune_mlp_modules=True,\n",
        "    bias=\"none\",\n",
        "    random_state=SEED,\n",
        ")\n",
        "print(\"🔧 LoRA adapters injected\")\n",
        "\n",
        "# ─────────── DATASET ─────────────────────────────────────────────────────────\n",
        "raw = Dataset.from_list(json.load(open(data_json)))\n",
        "def to_chat(ex):\n",
        "    return {\"text\":\n",
        "        f\"<bos><start_of_turn>user\\n{ex['question']}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n{ex['answer']}<end_of_turn>\"\n",
        "    }\n",
        "data = raw.map(to_chat)\n",
        "\n",
        "if SUBSET:\n",
        "    data = data.select(range(min(SUBSET, len(data))))\n",
        "train_ds, val_ds = data.train_test_split(0.2, seed=SEED).values()\n",
        "print(f\"📚 Train / val = {len(train_ds)} / {len(val_ds)}\")\n",
        "\n",
        "# ─────────── SFT ─────────────────────────────────────────────────────────────\n",
        "print(\"\\n🎯 Supervised fine‑tune …\")\n",
        "\n",
        "cfg = SFTConfig(\n",
        "    dataset_text_field          = \"text\",\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    num_train_epochs            = EPOCHS_SFT,\n",
        "    learning_rate               = LR,\n",
        "    warmup_steps                = 10,\n",
        "    logging_steps               = 10,\n",
        "    fp16                        = not is_bfloat16_supported(),\n",
        "    bf16                        = is_bfloat16_supported(),\n",
        "    optim                       = \"adamw_8bit\",\n",
        "    seed                        = SEED,\n",
        "    report_to                   = \"wandb\",\n",
        ")\n",
        "SFTTrainer(model=model, tokenizer=tok, train_dataset=train_ds,\n",
        "           eval_dataset=val_ds, args=cfg).train()"
      ],
      "metadata": {
        "id": "2glvFKvmDhDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────── SAVE ADAPTERS ──────────────────────────────────────────────────\n",
        "stamp       = now()\n",
        "adapter_dir = f\"hypnos_adapters_{stamp}\"\n",
        "model.save_pretrained(adapter_dir); tok.save_pretrained(adapter_dir)\n",
        "shutil.copytree(adapter_dir, f\"{drive_root}/{adapter_dir}\")\n",
        "print(\"💾  LoRA‑only checkpoint saved:\", adapter_dir)\n",
        "\n",
        "print(\"\\n🎉 Training complete\")\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "eGEra1y3FeAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning with AI Feedback (RLAIF) 🤖\n",
        "\n",
        "- Here we use the Direct Policy Optimisation RLAIF approach from Unsloth"
      ],
      "metadata": {
        "id": "9noUHN9HHm6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastModel, PatchDPOTrainer, is_bfloat16_supported\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import Dataset\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Common GOTCHA with recursion limit issue in Unsloth\n",
        "torch._dynamo.config.cache_size_limit = 128\n",
        "# Patch Unsloth's DPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "drive_root   = \"/content/gdrive/...\"\n",
        "adapter_dir  = f\"/content/hypnos_adapters_20250802_081519\"                      # ← your saved LoRA dir\n",
        "base_model_id    = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "DPO_DATASET  = f\"{drive_root}/data/sleep-train-dpo-enriched.json\"               # replace with your DPO JSON file\n",
        "max_samples = 0         # Test subset by setting to >0\n",
        "\n",
        "\n",
        "# 1. Load base model + tokenizer\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    base_model_id,                     # same base used during original SFT\n",
        "    load_in_4bit=False,\n",
        "    full_finetuning=False,\n",
        "    max_seq_length=2048\n",
        ")\n",
        "\n",
        "# 2. Inject LoRA structure into the model (important!)\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    finetune_language_layers=True,\n",
        "    finetune_attention_modules=True,\n",
        "    finetune_mlp_modules=True,\n",
        "    bias=\"none\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "# 3. Load previously fine-tuned LoRA weights (SFT checkpoint)\n",
        "model.load_adapter(adapter_dir, adapter_name=\"default\")\n"
      ],
      "metadata": {
        "id": "MSy9qefuVgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# === Load and validate your preference data ===\n",
        "with open(DPO_DATASET) as f:\n",
        "    preference_data = json.load(f)\n",
        "\n",
        "# ✅ Rename \"question\" → \"prompt\"\n",
        "for sample in preference_data:\n",
        "    sample[\"prompt\"] = sample.pop(\"question\")\n",
        "\n",
        "print(f\"📊 Created {len(preference_data)} preference pairs\")\n",
        "if preference_data:\n",
        "    print(f\"📋 Sample preference pair:\\n{json.dumps(preference_data[0], indent=2)}\")\n",
        "else:\n",
        "    raise ValueError(\"❌ No valid preference pairs found!\")\n",
        "\n",
        "# === Create Hugging Face dataset ===\n",
        "preference_dataset = Dataset.from_list(preference_data)\n",
        "print(f\"✅ Preference dataset created with {len(preference_dataset)} examples\")\n",
        "\n",
        "# === Get correct chat template for your tokenizer (adjust template if needed) ===\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"gemma-3\")\n",
        "\n",
        "# === Apply DPO formatting ===\n",
        "print(\"🔄 Formatting preference dataset for DPO...\")\n",
        "\n",
        "def apply_dpo_template(example):\n",
        "    # Validate all required fields\n",
        "    if not all(isinstance(example[field], str) and example[field].strip() for field in [\"prompt\", \"chosen\", \"rejected\"]):\n",
        "        return {\n",
        "            \"prompt\": \"\",\n",
        "            \"chosen\": \"\",\n",
        "            \"rejected\": \"\"\n",
        "        }\n",
        "\n",
        "    # Format using chat template\n",
        "    return {\n",
        "        \"prompt\": tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": example[\"prompt\"]}], tokenize=False),\n",
        "        \"chosen\": tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": example[\"prompt\"]}, {\"role\": \"assistant\", \"content\": example[\"chosen\"]}], tokenize=False),\n",
        "        \"rejected\": tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": example[\"prompt\"]}, {\"role\": \"assistant\", \"content\": example[\"rejected\"]}], tokenize=False)\n",
        "    }\n",
        "\n",
        "# Apply formatting (no rename step needed)\n",
        "preference_dataset = preference_dataset.map(\n",
        "    apply_dpo_template,\n",
        "    remove_columns=preference_dataset.column_names,\n",
        "    desc=\"🧠 Applying chat template formatting\"\n",
        ")\n",
        "\n",
        "# Optional sanity check\n",
        "print(\"✅ Final formatted sample:\")\n",
        "print(preference_dataset[0])\n",
        "\n",
        "\n",
        "if max_samples > 0:\n",
        "  preference_dataset = preference_dataset.select(range(max_samples))\n",
        "\n",
        "split_dataset = preference_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "8876xHJZVMpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────── DPO CONFIG ─────────────────────────────────────────────────────\n",
        "config = DPOConfig(\n",
        "    beta                         = 0.01,  # DPO trade-off factor\n",
        "    per_device_train_batch_size  = 2,\n",
        "    per_device_eval_batch_size   = 2,\n",
        "    gradient_accumulation_steps  = 8,\n",
        "    eval_steps                   = 10,\n",
        "    logging_steps                = 5,\n",
        "    num_train_epochs             = 1,\n",
        "    learning_rate                = 1e-5,\n",
        "    lr_scheduler_type            = \"cosine\",\n",
        "    warmup_ratio                 = 0.05,\n",
        "    optim                        = \"adamw_8bit\",\n",
        "    seed                         = 3407,\n",
        "    bf16                         = is_bfloat16_supported(),\n",
        "    fp16                         = not is_bfloat16_supported(),\n",
        "    report_to                    = \"none\"\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model           = model,\n",
        "    ref_model       = None,      # uses frozen copy of model by default\n",
        "    args            = config,\n",
        "    beta            = config.beta,\n",
        "    tokenizer       = None,\n",
        "    train_dataset   = preference_dataset,\n",
        "    max_length      = 2048,\n",
        "    evaluation_strategy=\"steps\",     # or \"epoch\"\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ycoy9cBNsSTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────── SAVE DPO LoRA ───────────────────────────────────────────────────\n",
        "stamp       = now()\n",
        "dpo_dir     = f\"dpo_hypnos_adapters_{stamp}\"\n",
        "model.save_pretrained(dpo_dir); tokenizer.save_pretrained(dpo_dir)\n",
        "shutil.copytree(dpo_dir, f\"{drive_root}/{dpo_dir}\")\n",
        "print(\"💾  DPO-finetuned LoRA checkpoint saved:\", dpo_dir)\n",
        "\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "w0v77i-JVofD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference 🏎\n",
        "- Lightweight inference testing to ensure the models were trained / saved / loaded correctly"
      ],
      "metadata": {
        "id": "kUgIPbjAIdsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# Paths\n",
        "BASE_REPO   = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n",
        "ADAPTER_DIR = \"/content/dpo_hypnos_adapters_20250802_114929\"\n",
        "\n",
        "# Load base + adapter\n",
        "model, tok = FastModel.from_pretrained(BASE_REPO, load_in_4bit=False)\n",
        "model.load_adapter(ADAPTER_DIR)\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "# Sample prompt\n",
        "prompt = \"<bos><start_of_turn>user\\nHow can I improve my sleep naturally?<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tok(text=prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode result\n",
        "response = tok.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\n📤 Response:\\n\", response)\n"
      ],
      "metadata": {
        "id": "RlShYgghIjL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting & Quantization 🗳"
      ],
      "metadata": {
        "id": "EMiC-CPDXqmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "GsXZc_8sX1Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "4A08HXzsX20h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to build llamacpp now to enable quantization features\n",
        "\n",
        "Note: Can try `cmake --build . --target quantize --config Release`\n",
        "\n",
        "For a faster, targeted build.\n",
        "\n",
        "⚠ But it does not always work across llamacpp versions"
      ],
      "metadata": {
        "id": "JhF2pIB9fh4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# [1] Update and install dependencies\n",
        "sudo apt update\n",
        "sudo apt install -y build-essential cmake\n",
        "\n",
        "# [2] Build llama.cpp\n",
        "cd /content/llama.cpp\n",
        "\n",
        "mkdir build\n",
        "cd build\n",
        "cmake ..\n",
        "cmake --build . --config Release"
      ],
      "metadata": {
        "id": "Orz6eayiesdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure quantization block is installed\n",
        "import os\n",
        "\n",
        "path = \"/content/llama.cpp/build/bin/llama-quantize\"\n",
        "\n",
        "if os.path.isfile(path) or os.path.isdir(path):\n",
        "    print(f\"✅ Quantization module successfully located...\")\n",
        "else:\n",
        "    print(f\"❌ File or directory does not exist: {path}\")\n"
      ],
      "metadata": {
        "id": "kommxHC6fNsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting to GGUF & Quantizing"
      ],
      "metadata": {
        "id": "u7zzYoyJYM1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_REPO   = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"      # Set to None if you don't want to save the base model\n",
        "BASE_MODEL_PATH = \"gemma-3n-base-model\"\n",
        "SAVE_BASE_MODEL = False\n",
        "ADAPTER_DIR = \"/content/hypnos_adapters_20250802_081519\"          # Your SFT Adapter\n",
        "DPO_ADAPTER_DIR = \"/content/dpo_hypnos_adapters_20250802_114929\"  # Your DPO Adapter\n",
        "\n",
        "unquantized_sft_gguf = f\"{ADAPTER_DIR}_unquantized.gguf\"\n",
        "quantized_sft_gguf = f\"{ADAPTER_DIR}_quantized.gguf\"\n",
        "\n",
        "unquantized_dpo_gguf = f\"{DPO_ADAPTER_DIR}_unquantized.gguf\"\n",
        "quantized_dpo_gguf = f\"{DPO_ADAPTER_DIR}_quantized.gguf\"\n",
        "\n",
        "quantization = \"q4_k_m\"\n",
        "\n",
        "\n",
        "def load_base_model():\n",
        "  model, tokenizer = FastModel.from_pretrained(\n",
        "      model_name = BASE_REPO,\n",
        "      dtype = None, # None for auto detection\n",
        "      max_seq_length = 1024, # Choose any for long context!\n",
        "      load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "      full_finetuning = False,\n",
        "  )\n",
        "  return model, tokenizer\n"
      ],
      "metadata": {
        "id": "zHEF_D0IXsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "if SAVE_BASE_MODEL:\n",
        "  model, tokenizer = load_base_model()\n",
        "  model.save_pretrained(BASE_MODEL_PATH)  # Local saving\n",
        "  tokenizer.save_pretrained(BASE_MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "ptu8CwM0YT80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAVE_BASE_MODEL:\n",
        "  input_model_path = f\"/content/{BASE_MODEL_PATH}\"\n",
        "  output_model_path = f\"/content/{BASE_MODEL_PATH}.gguf\"\n",
        "  !python /content/llama.cpp/convert_hf_to_gguf.py {input_model_path} --outfile {output_model_path}"
      ],
      "metadata": {
        "id": "87xQQigKap1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "for tuned_model_dir in [ADAPTER_DIR, DPO_ADAPTER_DIR]:\n",
        "  model, tokenizer = load_base_model()\n",
        "\n",
        "  model = FastModel.get_peft_model(\n",
        "      model,\n",
        "      r=16,\n",
        "      target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "      lora_alpha=32,\n",
        "      lora_dropout=0.05,\n",
        "      bias=\"none\",\n",
        "  )\n",
        "\n",
        "  # Load the LoRA adapter weights with adapter name\n",
        "  model.load_adapter(tuned_model_dir, adapter_name=\"default\")\n",
        "\n",
        "  # Step 1: Save as full model in proper HF format\n",
        "  full_model_folder = f\"{tuned_model_dir}_merged\"\n",
        "\n",
        "  print(f\"💾 Saving full model to {full_model_folder}...\")\n",
        "  model.save_pretrained_merged(full_model_folder, tokenizer)"
      ],
      "metadata": {
        "id": "XZQMQIv5YhKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Convert to unquantized GGUF using llama.cpp\n",
        "print(f\" Converting to unquantized GGUF...\")\n",
        "\n",
        "sft_full_folder = f\"{ADAPTER_DIR}_merged\"\n",
        "dpo_full_folder = f\"{DPO_ADAPTER_DIR}_merged\"\n",
        "\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py {sft_full_folder} --outfile {unquantized_sft_gguf}\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py {dpo_full_folder} --outfile {unquantized_dpo_gguf}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hubi77wBb9ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run quantization on the extracted model\n",
        "# Using raw bash here vs. subprocess as this might take a while and log output is crucial\n",
        "! \"/content/llama.cpp/build/bin/llama-quantize\" {unquantized_sft_gguf} {quantized_sft_gguf} {quantization}\n",
        "! \"/content/llama.cpp/build/bin/llama-quantize\" {unquantized_dpo_gguf} {quantized_dpo_gguf} {quantization}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mZ3O5Y0ZeCDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the files permanently on the mounted drive"
      ],
      "metadata": {
        "id": "dhVF06cgd6dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify local + mounted paths\n",
        "import os\n",
        "drive_dir = \"/content/gdrive/MyDrive/Files/Personal/Hypnos-Project/content/models\"\n",
        "BASE_MODEL_FILENAME = f\"{BASE_MODEL_PATH}.gguf\"\n",
        "SRC_MODEL_FILENAMES = [unquantized_sft_gguf, quantized_sft_gguf, unquantized_dpo_gguf, quantized_dpo_gguf]\n",
        "MODEL_FILENAMES = [str(os.path.basename(i)) for i in SRC_MODEL_FILENAMES]\n",
        "print(\"Base model filename: \" + BASE_MODEL_FILENAME)\n",
        "print(\"Output model filename: \" + str(MODEL_FILENAMES))\n",
        "\n",
        "BASE_MODEL_GGUF_PATH_OUTPUT = f\"{drive_dir}/{BASE_MODEL_FILENAME}\"\n",
        "DST_MODEL_FILENAMES = [f\"{drive_dir}/{i}\" for i in MODEL_FILENAMES]\n",
        "\n",
        "# Copy models over to mounted dir\n",
        "! cp {BASE_MODEL_FILENAME} {BASE_MODEL_GGUF_PATH_OUTPUT}\n",
        "\n",
        "for (src, dest) in zip(SRC_MODEL_FILENAMES, DST_MODEL_FILENAMES):\n",
        "  print(f\"Copying model {src} over to {dest}...\")\n",
        "  ! cp {src} {dest}"
      ],
      "metadata": {
        "id": "wxt19vTxd40l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You're all done! 🎉 🎉\n",
        "\n",
        "You should now have base, SFT, and DPO gguf models exported to your GDrive. 👏\n",
        "\n",
        "Proceed to the Evaluation notebook to test out the extracted gguf... 🏃🔧"
      ],
      "metadata": {
        "id": "eUmm4XIpdqXA"
      }
    }
  ]
}